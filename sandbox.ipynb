{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = open('unijour.md') #, mode=\"r\", encoding=\"utf-8\")\n",
    "text = input_file.read()\n",
    "# text\n",
    "html = markdown.markdown(text)\n",
    "with open('view.html', mode='w') as F:\n",
    "    F.write( html )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "with open('view.html', mode='r') as F:\n",
    "    html = F.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "# level = 2\n",
    "# headings = soup.find_all(f'h{level}')\n",
    "# soup.contents\n",
    "# headings\n",
    "# for link in soup.find_all('h2'):\n",
    "#     print(link.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<h2>modified: 2020-03-09T22:39:10+01:00</h2>, '\\n', <h1>unijour</h1>, '\\n', <h1>22.2.20</h1>, '\\n'], [<h2>Backpropagation of logistic function</h2>, '\\n', <p>The forward function is\n",
      "$$\n",
      "f(x) = \\frac{1}{1+e^{-Wx}},\n",
      "$$\n",
      "where $W$ is the vector containing the model coefficients and $\\mathbf x = [1, x]$ is the basis of the one-dimensional data.  $W = [a_0, a_1]$ and the exponent is $W\\cdot \\mathbf x$.</p>, '\\n', <p>To update $W$, I think we'll use backpropagation. First we feed data into the $f(x)$. This maps the relevant independent variable $\\mathbf x$ to a $y$, which we model by $f(x)$. </p>, '\\n', <p>Then we somehow have to update $W$, which I think we can do by gradient descent. For gradient descent we'll need $f'(x)$:\n",
      "$$\n",
      "f'(x) = W \\frac{e^{-Wx}}{(1+e^{-Wx})^2}  = W f(x)(1-f(x)).\n",
      "$$\n",
      "Notice that the minus sign drops out because the minus sign from the exponential cancels the one from the fraction. This is consistent with the results I found online. We can then use gradient descent to relate the derivative $f'(x)$ to the update for $W$:\n",
      "$$\n",
      "W_{n+1} = W_n - \\gamma \\nabla_W F(W),\n",
      "$$\n",
      "where $F(W)$ is the actual function being minimized. I think we should minimize the square error, so\n",
      "$$\n",
      "F(W) = \\sum_i ( f(\\mathbf x_i) - y <em>i )^2,\n",
      "$$\n",
      "where $y_i$ is the label (i.e. the classifier), and $f(\\mathbf x _i)$ is the logistic function with the features inputted into it. Since $y_i$ does not depend on $W$, we find\n",
      "$$\n",
      "\\nabla_W F(W) = 2 \\sum_i (f(\\mathbf x _i) - y_i) \\nabla_W f(\\mathbf x_i)\n",
      " = 2 \\sum_i (f(\\mathbf x _i) - y_i) \\mathbf x_i f(\\mathbf x_i)(1-f(\\mathbf x_i)).\n",
      "$$\n",
      "So if all of this is correct, we end with an update step \n",
      "$$\n",
      "W</em>{n+1} = W_n - \\gamma \\nabla_W F(W), \\\n",
      "\\nabla_W F(W) =  2 \\sum_i (f(\\mathbf x _i) - y_i) \\mathbf x_i f(\\mathbf x_i)(1-f(\\mathbf x_i)).\n",
      "$$\n",
      "Although it might seem like we could re-use most of the data in the $\\nabla_W F(W)$, since $f(\\mathbf x _i )$ actually changes in the process of updating the $W$, I think we have to recalculate this summation every time. Which gets quite expensive, as it scales with the data set size. I'm not sure if I'm doing this correctly tbh.</p>, '\\n', <p>As you can see below, this algorithm seems to work quite well :) The data is a logistic function itself, with noise added and a post-hoc ceiling function to map it to the discrete set {0,1}.</p>, '\\n', <p><img alt=\"download\" src=\"unijour_figs/download.png\"/></p>, '\\n', <hr/>, '\\n', <p><strong>So here's the summary of the process:</strong></p>, '\\n', <ol>\n",
      "<li>we start with model coefficients $W = [\\beta_0, \\beta_1]$</li>\n",
      "<li>data with features $x_i$ and corresponding labels $y_i$ are given</li>\n",
      "<li>the $(x_i,y_i)$ pairs are fed into a logistic function $f(x)$</li>\n",
      "<li>we compute the MSE $F$</li>\n",
      "</ol>, '\\n', <h1>23.2.20</h1>, '\\n'], [<h2>Discussion with Andreas about friendship</h2>, '\\n', <p>Based on the conversation with Andreas, I noticed that I expect from Joris that:</p>, '\\n', <ul>\n",
      "<li>I expect from him a similar commitment and emotional investment as I am to make</li>\n",
      "<li>concerning skills/abilities, I demand or expect from people--in general--a large amount of:</li>\n",
      "<li>emotional intelligence</li>\n",
      "<li>ability to communicate both on intellectual topics as well as emotional topics</li>\n",
      "<li>willingness/ability to change their minds</li>\n",
      "</ul>, '\\n', <p>Now, it is important to note that these traits that I value are all traits that I score rather high on. Coincidence? Probably not. This is of course a chicken/egg story--things that I find important, I more likely will work on.</p>, '\\n', <p>Are these <em>reasonable</em> things to demand from people? Concerning commitment and willingness to invest in a friendship, this I value much stronger than most people. So, although it is a good trait to have, it maybe is counterproductive to demand this from everyone. So the following two statements can be simultaneously true:</p>, '\\n', <ul>\n",
      "<li><em>demand</em> strong development in the above traits</li>\n",
      "<li><em>value</em> strong development in the above traits</li>\n",
      "</ul>, '\\n', <p>Andreas referred to an 'Aristotelean concept' of friendship, where we are with people for their virtue, not their development--even if that development pertains to emotional skills, I suppose?</p>, '\\n', <p><strong>Summary:</strong> I am now more aware of what I expect from a friendship, and that a reframing of these expectations might be helpful.</p>, '\\n', <hr/>, '\\n'], [<h2>Reliability, predictability and trust</h2>, '\\n', <p>During our deep reflection Sundays, Andreas mentioned that he has few very stable pillars in his life. He mentioned that this makes him feel emotionally unstable. This, I connected to the notion of <em>predictablity</em> (in management that the \"Consequential Leadership\" lecture from Thursday):</p>, '\\n', <blockquote>\n",
      "<p><strong>Trust</strong> needs <strong>predictability</strong> of action. <strong>Unpredictable</strong> behavior is associated with <strong>distrust</strong>.</p>\n",
      "</blockquote>, '\\n', <hr/>, '\\n', <h1>24.2.20</h1>, '\\n'], [<h2>Small pressures around chest area [health]</h2>, '\\n', <p>Since 2-3 days now, I can feel small pressures around my chest area. At the bottom of my breasts, it feels like its a few cm deep into my body (i.e. not on my skin). At first I thought it was localized to my heart, but now I can tell it's also on the right hand side of my torso sometimes. Yesterday night, I bent over a few times, which seemed to trigger the pressure. I don't recall having this pressure ever before in my life.</p>, '\\n', <p>I would characterize it by the following properties:</p>, '\\n', <ul>\n",
      "<li>quite mild (I think I had to be aware of it to easily notice it)</li>\n",
      "<li>not painful, also not unpleasant necessarily</li>\n",
      "<li>haven't noticed such a senseation before</li>\n",
      "</ul>, '\\n', <h1>24.2.20</h1>, '\\n'], [<h2>Upskilling in tech with Federico</h2>, '\\n', <p>Micro-skill I'd like to practice with Federico:</p>, '\\n', <ol>\n",
      "<li>git</li>\n",
      "<li>regex</li>\n",
      "<li>bash</li>\n",
      "<li>ssh</li>\n",
      "</ol>, '\\n', <p>less obviously useful for both of us:</p>, '\\n', <ol>\n",
      "<li>SQL (quite likely useful).. Federico can prepare by studying concept of SQL or K can try to explain on-the-fly</li>\n",
      "<li>docker</li>\n",
      "<li>selected design principles</li>\n",
      "<li>unit testing?</li>\n",
      "<li>markdown</li>\n",
      "</ol>, '\\n', <h1>25.2.20</h1>, '\\n'], [<h2>EA Career Meeting</h2>, '\\n', <ul>\n",
      "<li>five min review at end of day .. with metrics, like \"how many hours did I write\" ?  -&gt; what's the purpose of the detailed past calendar</li>\n",
      "<li>time tracking in the past may be useful to:</li>\n",
      "<li>see blindspots, e.g. points that I'm spending a lot of time but that are maybe not so meaningful to me</li>\n",
      "<li>do better estimation for the future</li>\n",
      "<li>cool book tip: Hari, lost connections</li>\n",
      "<li>Yannik Bauer -&gt; procrastination is also partly an <em>emotional state</em>, i.e. there might be emotions at play why we're procrastinating... investigating those can be useful</li>\n",
      "<li>Peter does <em>more</em> exercise when he needs to be very productive. He works out an hour extra a day, enabling him to work up to 14 hrs a day.</li>\n",
      "<li>mindfulness in itself is all about being <em>aware of feelings</em>, not necessarily directly understanding them. However, this mindfulness can allow for getting more objective</li>\n",
      "</ul>, '\\n', <h1>29.02.20</h1>, '\\n'], [<h2>Idea from David to improve finishing off tasks</h2>, '\\n', <p>Get a small reward for every kanban card like dark chocolate nuts.</p>, '\\n'], [<h2>Ideas from David to increase marketing for critical thinking</h2>, '\\n', <ul>\n",
      "<li>Twitter account - &gt;     pass out Twitter handle  at events </li>\n",
      "<li>making clear what value it brings to potential students</li>\n",
      "</ul>, '\\n', <h1>01.03.20</h1>, '\\n'], [<h2>Finishing off assessing scale Lyzeum class.</h2>, '\\n', <p>Most important concept:</p>, '\\n', <ul>\n",
      "<li>base rate\n",
      "  Side concepts:</li>\n",
      "<li>black swans</li>\n",
      "</ul>, '\\n', <p>Other important topic should be bringing together the different ways of estimating scale of the past few classes. We estimated extremes using the CLT, discussed breakdown of the CLT because of correlated events. We discussed how then eztremes can be set by the largest scale in the system. We also discussed thinking of more representative cases, and how our brain often mistakes representativeness for availability.</p>, '\\n', <p>So we could imagine a matrix like this:\n",
      "    Type of estimate   - &gt;\n",
      "Validity \n",
      " |\n",
      " v                extreme          non extreme\n",
      "Correct\n",
      "Flawed\n",
      "But then I'd have to make more clear what kind of systems this concerns, bc the extreme using CLT only applies to systems with not highly correlated variables. I'm not sure if I can make this matrix complete without making it very case-dependent.</p>, '\\n', <p>Long story short, it should be clear to the students how these different types of estimation relate to each other.</p>, '\\n', <p>Would be nice to tie this in to black swans as well, and how estimation can go horribly wrong.</p>, '\\n', <h1>02.03.20</h1>, '\\n'], [<h2>Arbeitsamt stuff</h2>, '\\n', <p>Just got back from the Arbeitsamt, Einfuehrungsgespraech with Frau Taut. This week I'll have to finish my online profile, with skills and such. It's not totally clear to me what value this brings me, as I don't think their 'Jobbörse' will bring me much value -- but I promised her, and I got the feeling it was compulsary for me. So I'll do it, even if it brings me no value. This has to happend <strong>by Friday the 6th</strong>.</p>, '\\n', <p>Also, Frau Taut said that they'd happily reimburse a German course, but <em>only</em> if that course takes place every day. Additinally, the company has to be certified. So I'll have to see whether that's possible/worth it. Before it can get reimbursed, I apparently need to be 'eingestuft' with a test. The bfz can/has to do this. Just looked at the ratings on Google Maps, and the bfz in Poccistrasse has surprisingly good ratings (4.0), especially for a government institution. So I might consider that.</p>, '\\n', <p>In terms of applications, I will have to start doing 1-2 per week, and make a big list of that which I can e-mail once every 1-2 months. On the 4th of April, I'll have to send my first one.</p>, '\\n'], [<h2>Some notes from deep reflection with Andreas dd 2.3.20</h2>, '\\n', <p>We talked about:</p>, '\\n', <ul>\n",
      "<li>Budgetting, Andreas had a nice isight that he can use to improve his budgetting</li>\n",
      "<li>how I'll be entering a few months that will require more 'steering' for me -- I will have to be able to keep more pots on the fire than ever before in my life</li>\n",
      "</ul>, '\\n', <p>and I now realize that I forgot most of the things we talked about... so in the future I should maybe write down notes right after the meeting</p>, '\\n'], [<h2>Plan for today</h2>, '\\n', <p>I would like to swallow as many cold frogs today as possible. That entails:</p>, '\\n', <ul>\n",
      "<li>[x] updating my BAfA profile</li>\n",
      "<li>[x] Start a trello board with the big picture projects for the upcoming few months</li>\n",
      "<li>[x] contact Martin about the small EA career meeting on the 9th and what I'd need to do for that</li>\n",
      "<li>[x] add projects to SquareSpace</li>\n",
      "</ul>, '\\n', <h1>3.3.20</h1>, '\\n'], [<h2>Journaling about yesterday @journaling</h2>, '\\n', <p>Yesterday was my first gym-day ever! I really enjoyed it -- the challenge, the feeling of muscle soreness, being high on dopamine ;P I'm excited to move forward with a regular subscription and train regularly with Valerie. I could also notice I'm starting to again be quite attracted to her. She's a bit more of a quiet person (although she's also young; that might change), but she's an impressive, strong person. During one of the exercises, we had to place our backs up against an elastic curved surface, and I could tell she was really straining herself to do those exercises. So it's not just that she's fit -- she's putting her body to the limit to get there. Besides being a very beautufil woman, I find that personal quality very attractive.</p>, '\\n', <p>Some more good news: I really enjoyed how productive I was yesterday. I got so many things done! There are several key differences now in the way that I work:</p>, '\\n', <ul>\n",
      "<li><strong>I no longer procrastinate endlessly</strong>. There are still some blindspots where I do procrastinate a little bit, such as signing up for lindiehop or improv (which would take me out of my comfort zone), but that's now only a small part of my life instead of the majority.</li>\n",
      "<li><strong>I work on important things first, instead of working on <em>fun</em> things first.</strong> Yesterday I tried to sense which things are important, and which things I enjoy least. Instead of working on the things I enjoy the most first, I like to work on them last now. That makes the overall process much less guilt-ridden, and more effective.</li>\n",
      "<li><strong>I have more clarity</strong>. Yesterday I made a 'project master board', with all the different major things going on in my life, and how much time they take. This gives me a lot of clarity, and makes me understand my current situation and gain more control over it.</li>\n",
      "</ul>, '\\n', <p>What shall I make the focus of my day from now on (post-lunch)? I think it would be nice if I could finish some key elements of the statistics lecture, so that I can focus on my first Kaggle competition (titanic) tomorrow. In particular, I'd like to: </p>, '\\n', <ul>\n",
      "<li>[x] Derive student's t distribution</li>\n",
      "<li>[ ] make notes for what likelihood means and how to compute it, given a model</li>\n",
      "</ul>, '\\n'], [<h2>Difficulty vs utility thinking tool w Federico</h2>, '\\n', <p>When thinking of the added value of projects, we can imagine a two-dimensional plot with difficulty $D$ (conditioned on current ability $A(t)$) $D(t, A(t))$ vs. added value $U(t)$ (conditioned on the current market). A full project would then be a path through this space, as a function of time.</p>, '\\n'], [<h2>How the student's t distribution emerges (contains mistake/is incomplete)</h2>, '\\n', <p>The student's t-distribution is\n",
      "$$\n",
      "p(t) = p(\\frac{\\sum_i x_i }{\\sqrt n s}),\n",
      "$$\n",
      "where $s^2 = \\sum_i x_i^2 / (n-1)$. To get to $p(t)$, we need to go through the following steps:</p>, '\\n', <ol>\n",
      "<li><strong><em>Task:</em></strong> Compute $p(\\sum_i x_i | \\sigma) $.\n",
      "   <strong><em>Method 1 (common)</em></strong>: We typically <em>assume</em> that $p(x_i)$ is a normal distribution with variance $\\sigma^2$, so that $p(\\sum_i x_i / n) = \\mathcal{N}(0, \\sigma^2 / n)$.</li>\n",
      "</ol>, '\\n', <p><strong><em>Method 2 (general):</em></strong> We can find a more general rule for the distribution of a sum by recognizing that the characteristic function satisfies\n",
      "   $$\n",
      "   \\phi_{U+V}(k) = \\langle \\exp(ik(U+V)) \\rangle = \\langle \\exp(ikU) \\rangle\\langle \\exp(ikV) \\rangle = \\phi_U (k) \\phi_V (k),\n",
      "   $$\n",
      "   and that multiplication in $k-$space corresponds to convolution in real space, so that $p(z=U+V) = (p(U)*p(V))(z)$. So if we know $p(U),p(V)$, then $p(z)$ can formally straightforwardly be found (although the convolution might be a messy integral to compute).</p>, '\\n', <ol>\n",
      "<li>\n",
      "<p>~~<strong><em>Task</em>:</strong> Compute $p(s)$. <strong><em>Method:</em></strong>~~</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>~~Compute $p(y=x^2)$. This can be found using the conservation of probability, $|p(y) d y| = p(x)dx$. From this follows that $p(y=x^2) = \\frac{1}{\\sqrt{2\\pi y}}e^{-y/2}$ .~~</p>\n",
      "</li>\n",
      "<li>~~Now that we have $p(y = x^2)$, we need to find $p(\\sum_i y_i)$. For this, we use the convolution theorem for summed variables that we found before.~~</li>\n",
      "<li>\n",
      "<p>~~We now need to find $p(s={\\sqrt z}^{-1})$ , which can again be found by using conservation of probability.~~</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong><em>Task:</em></strong> Compute $p(\\sqrt n \\bar x / s)$. <strong><em>Method:</em></strong></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>We can marginalize $p(\\mu, \\sigma^2 | D)$ to find $p(\\mu|D)$ by:\n",
      "      $$\n",
      "      \\begin{aligned}\n",
      "      p(\\mu | D) &amp; = \\int d\\sigma^2 p(\\mu, \\sigma^2 | D) \\\n",
      "       &amp; = \\int d\\sigma^2 p(\\mu | \\sigma^2,  D)  p(\\sigma^2 | D)\n",
      "      \\end{aligned}\n",
      "      $$</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>For the both factors in the integrand, we use Bayes' theorem  $p(\\mu|\\sigma^2,D) = p(D|\\mu,\\sigma^2)p(\\mu|\\sigma^2) / p(D|\\sigma^2)$ and  $p(\\sigma^2|D) = p(D|\\sigma^2) p(\\sigma^2) / p(D)$. This gives\n",
      "      $$\n",
      "      p(\\mu|D) \\sim \\int d\\sigma^2 p(D|\\mu,\\sigma^2)p(\\mu|\\sigma^2)p(\\sigma^2),\n",
      "      $$\n",
      "      where we use the uninformative prior for $p(\\sigma^2)$ which has to scale as  $p(\\sigma^2) \\sim \\sigma^{-2}$ for dimensional validity. The first factor is trivial: it is simply the likelihood of the data under the model. For $p(\\mu|\\sigma^2)$, we use the normal distribution found in step 1: $p(\\mu|\\sigma^2) =  \\mathcal{N}(0, \\sigma^2 / n)$.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>Compute the integral. This results in the Student's <em>t</em>-distribution.</p>\n",
      "</li>\n",
      "</ol>, '\\n', <h1>4.3.20</h1>, '\\n'], [<h2>Plan for today</h2>, '\\n', <ul>\n",
      "<li>[x] reminder EA meeting on slack (20)</li>\n",
      "<li>[x] reminder found princ of stat lecture w room (20)</li>\n",
      "<li>[x] correct student t-distribution derivation (40)</li>\n",
      "<li>[x] draw general flow chart for found princ of stat lecture (120)</li>\n",
      "<li>[x] what is the likelihood functino (45)</li>\n",
      "<li>[x] devise overall structure of lecture (120)</li>\n",
      "<li>[x] think of cool examples of when we'd need hypothesis testing in statistics (30)</li>\n",
      "</ul>, '\\n'], [<h2>Call with Martin about career meeting</h2>, '\\n', <p>Just now had a very long call with Martin about the EA career meeting, which I think didn't go so well. Martin and I have quite fundamental differences of opinion: Martin likes to seek consensus in groups, and isn't comfortable with two or three people setting out the course, and I much more prefer to have clarity and to remove long discussions with an unclear vision. I think I like it when people take charge and decisions are made quickly, and he likes it when people agree and are on board with the vision. Obviously, some kind of balance between these two extremes should always be struck, but he and I are on different points on this spectrum it seems.</p>, '\\n', <p>Anyway, the current plan is that maybe/probably Andreas and I prepare some kind of feedback form or proposal for the purpose of the upcoming meetings. Martin proposed that this smaller group then condenses this information down to a proposal, and that we make a group-based decision in a month from now.</p>, '\\n'], [<h2>How the student's t distribution emerges</h2>, '\\n', <p>The student's t-distribution is\n",
      "$$\n",
      "p(t) = p(\\frac{\\sum_i x_i }{\\sqrt n s}),\n",
      "$$\n",
      "where $s^2 = \\sum_i x_i^2 / (n-1)$. To get to $p(t)$, we need to go through the following steps:</p>, '\\n', <ol>\n",
      "<li><strong><em>Task:</em></strong> Compute $p(\\sum_i x_i | \\sigma) $.\n",
      "   <strong><em>Method 1 (common)</em></strong>: We typically <em>assume</em> that $p(x_i)$ is a normal distribution with variance $\\sigma^2$, so that $p(\\sum_i x_i / n) = \\mathcal{N}(0, \\sigma^2 / n)$.</li>\n",
      "</ol>, '\\n', <p><strong><em>Method 2 (general):</em></strong> We can find a more general rule for the distribution of a sum by recognizing that the characteristic function satisfies\n",
      "   $$\n",
      "   \\phi_{U+V}(k) = \\langle \\exp(ik(U+V)) \\rangle = \\langle \\exp(ikU) \\rangle\\langle \\exp(ikV) \\rangle = \\phi_U (k) \\phi_V (k),\n",
      "   $$\n",
      "   and that multiplication in $k-$space corresponds to convolution in real space, so that $p(z=U+V) = (p(U)*p(V))(z)$. So if we know $p(U),p(V)$, then $p(z)$ can formally straightforwardly be found (although the convolution might be a messy integral to compute).</p>, '\\n', <ol>\n",
      "<li>\n",
      "<p><strong><em>Task</em>:</strong> Compute $p(s)$. <strong><em>Method:</em></strong></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>Compute $p(y=x^2)$. This can be found using the conservation of probability, $|p(y) d y| = p(x)dx$. From this follows that $p(y=x^2) = \\frac{1}{\\sqrt{2\\pi y}}e^{-y/2}$ .</p>\n",
      "</li>\n",
      "<li>Now that we have $p(y = x^2)$, we need to find $p(\\sum_i y_i)$. For this, we use the convolution theorem for summed variables that we found before.</li>\n",
      "<li>\n",
      "<p>We now need to find $p(s={\\sqrt z}^{-1})$ , which can again be found by using conservation of probability.</p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong><em>Task:</em></strong> Compute $p(\\sqrt n \\bar x / s)$. <strong><em>Method:</em></strong></p>\n",
      "</li>\n",
      "<li>\n",
      "<p>$p(t) = \\int ds ~ d\\bar x \\delta( t- \\sqrt n \\bar x / s) p(\\bar x) p(s)$</p>\n",
      "</li>\n",
      "<li>Using a variable substitution, we can express the above integral in terms of Gamma functions.</li>\n",
      "</ol>, '\\n'], [<h2>To do tomorrow</h2>, '\\n', <ol>\n",
      "<li>~~background of random forest~~</li>\n",
      "<li>~~three videos of Udemy course~~</li>\n",
      "<li>go for a walk</li>\n",
      "<li>hyperparameter tuning of random forest - how to do this</li>\n",
      "<li>(if tired of learning) finishing the statistics</li>\n",
      "<li>(if tired of learning) Bayesian lecture</li>\n",
      "<li><em>make plan for next day (Friday)</em></li>\n",
      "</ol>, '\\n', <h1>5.3.20</h1>, '\\n'], [<h2>Notes from strategizing call with Andreas @journaling</h2>, '\\n', <ul>\n",
      "<li>I realized that it's often difficult for me to show the same amount of empathy towards people I disagree with, or want something from, than otherwise. Two recnt cases: Joris not coming to my defence (and me starting to blame/analyze him), and Martin and how he had different ideas about the EA career meetings (and me not just accepting, but trying to convince him)</li>\n",
      "<li>During the upcoming few weeks, whenever I have a few free hours and I'm tired of studying DS, I can already work on my Bayesian statistics lecture --- despite that lecture being months away. Same goes for any Lyzeum lectures that I may have to give.</li>\n",
      "<li>I should regard upskilling in DS as my day job, and try to work ~ eight hours on this every day. This is very valuable time I have, being able to fully focus on upskilling. So I can make this time really wortwhile :)</li>\n",
      "</ul>, '\\n'], [<h2>Bias vs Variance Summary @data-science</h2>, '\\n', <p><strong>Bias</strong>: the consistent deviation of a model from the data, given arbitrarily many datapoints.\n",
      "<strong>Variance</strong>: Variation of the parameters across different sample sets from the same population</p>, '\\n', <p>It turns out that there is a <em>trade-off</em> between these two different sources of error: Reducing intermodel variability (decreasing bias) will result in an increase in the bias with the data. Under rather broad assumptions, does this trade-off exist:</p>, '\\n', <ul>\n",
      "<li><strong>independence of the underlying function $f$ we're trying to model on the different data-sets $D$.</strong> Although that <em>should</em> be the case, I think that if the data $D = D_1 \\cup D_2$, where $D_1,D_2$ emerged from two <em>different</em> systems (respectively $f_1,f_2$), then the independence of $f$ on $D$ is broken. This is not merely an academic issue, as we often don't really know what we're modeling exactly, so it might be the case that there are correlations between the function and dataset partitioning. Practical examples: mislabeling, with a mislabeling probability depending on $D$; data registration probability that explicitly depends on $D$; etc.</li>\n",
      "<li><strong>the noise has zero mean</strong>. I'm not sure if we can always do this. Suppose the noise is a one-sided distribution $\\epsilon \\geq 0$, $y = f(x) + \\epsilon$, then what we'd like to figure out is $f(x)$. We can define $f' = f(x) + \\langle \\epsilon (x) \\rangle$, so that $f = f'-\\langle\\epsilon\\rangle$ and $y = f'+\\epsilon'$ which trivially has zero-mean noise $\\epsilon' = \\epsilon-\\langle\\epsilon\\rangle$. We could then compute $f(x)$ as $f(x) = f'+\\langle\\epsilon\\rangle$, but we'd have to have access to the noise mean. I suppose we could estimate that, assuming that the model approaches $f$ sufficiently well, but our approximation for $\\epsilon$ will never be perfect.</li>\n",
      "<li><strong>finite noise variance</strong>. This can be broken for fat-tailed distributions, but might still allow for a sensible model.</li>\n",
      "<li><strong>statistical independence of the model on the noise: </strong> i.e. $\\langle \\epsilon \\hat f \\rangle = \\langle \\epsilon  \\rangle \\langle \\hat f \\rangle$, which I don't think holds if the error metric for learning the model is not the squared error.</li>\n",
      "</ul>, '\\n'], [<h2>Random Forest Motivation @data-science</h2>, '\\n', <p><strong>Goal: </strong> Perform classification or regression task. Since we train the decision tree on labeled data, this is almost always a supervised learning method.\n",
      "<strong>Naive approach:</strong> Decision tree. \n",
      "<strong>Problem with naive approach: </strong>Apparently very large variance across different decision trees for different datasets from the same population. One of the reasons for this overfitting is that the trees can become very complex.\n",
      "<strong>Solution 1 (bootstrap aggregating): </strong> Make an <em>ensemble</em> of decision trees for different training sets of <em>bagged data</em>[^what_is_bagging],</p>, '\\n', <p>[^what_is_bagging]: We have training data $X = {x_1,\\ldots, x_n}, Y = {y_1,\\ldots, y_n}$. We construct $B$ new training sets. For each $b = 1\\ldots B$, we construct a new training set as $X_b = {x^b_1,\\ldots, x^b_n}$ (and accompanying label), where each $x_i^b$ was sampled with replacement from $X$.</p>, '\\n', <p>where each of the trees may be overfit to their respective data, but the way in which they're overfit is randomly distributed across the different trees. If true, this would result in a 'forest' of trees, the <em>average</em> or <em>majority vote</em> prediction of the forest will be much less overfit than any single tree.\n",
      "<strong>Solution 2 (random subspace/random forest method): </strong> Not only are the <em>datapoints</em> randomly sampled, but at each node split, <em>features</em> are also randomly selected.\n",
      "<strong>Why does solution 1 decrease variance?: </strong>By sampling $B$ trees, the variance is reduced to $\\sigma^2 / B$ if the trees are independently sampled. For correlated trees (as is often the case, apparently), the variance is $\\rho \\sigma^2 + (1-\\rho)/B \\sigma^2$, where $\\rho $ is the pairwise correlation between two variables. Thus, for correlated trees, there is a lower bound on the total variance by bootstrapping/bagging.\n",
      "<strong>Why does solution 2 further decrease variance?: </strong>By randomly selecting features, we reduce the correlation $\\rho$. The further we reduce the number of features we select at each node split, the further $\\rho$ is decreased (but the information is also reduced, so I suspect this comes at the risk of underfitting).</p>, '\\n'], [<h2>Cool things learned from ZTM DS Udemy course (sklearn overview video) @data-science</h2>, '\\n', <ul>\n",
      "<li>pickling binary objects to save for future usage using Python's <code>pickle</code> library.</li>\n",
      "<li><code>train_test_split</code> in <code>sklearn</code>'s <code>model_selection</code></li>\n",
      "<li>in <code>sklearn</code>, each different algorithm (called a <em>classifier</em> in <code>sklearn</code>) is an object, with methods on that object being functions such as <code>fit</code>, <code>predict</code> etc.</li>\n",
      "<li>once an <code>sklearn</code> model is fitted, we can generate confusion matrices, model scores and classification reports with recall, support and other metrics.</li>\n",
      "</ul>, '\\n'], [<h2>ZTM DS Udemy course (car dataset) @data-science</h2>, '\\n', <ul>\n",
      "<li>\n",
      "<p>categorical features are transformed to numerical ones by <em>encode to one hot</em></p>\n",
      "</li>\n",
      "<li>\n",
      "<p><strong>imputation: </strong> missing features can be filled by e.g. the median of that respective column, or by \"missing\" for missing categorical features[^data_leakage] <code>sklearn</code> has two nice helper classes for this: <code>SimpleImputer</code> and <code>ColumnTransformer</code></p>\n",
      "</li>\n",
      "</ul>, '\\n', <p>[^data_leakage]: It's important to <em>never</em> have our test data impact our training data. This is called <em>data leakage</em>. Thus, the test and training sets have to be impuned <em>separately</em>.</p>, '\\n', <ul>\n",
      "<li>entries with missing labels have to be removed alltogether</li>\n",
      "</ul>, '\\n', <h1>6.3.20</h1>, '\\n'], [<h2>DS notes from self-study today</h2>, '\\n', <h3>Bootstrapping</h3>, '\\n', <p><em>Bootstrapping</em> is a method of constructing datasets sampled with replacement from the original dataset. For example, consider the tiny dataset ${ x_1, x_2 }$, then the set of all bootstrapped datasets is ${ x_1, x_1 },{ x_1, x_2 }, { x_2, x_1 },{ x_2, x_2 }$. These are called <em>bootstrapped datasets</em>. What we then do with these bootstrapped datasets is not specified within the term itself, but often it is used to estimate the variance of the model with respect to different datasets:\n",
      "$$\n",
      "\\mathrm{var} [ S(Z) ] = \\dfrac{1}{B-1} \\sum_b (S(Z^b) - \\bar S)^2,\n",
      "$$\n",
      "where $Z^b$ is the $b-$th bootstrapped dataset. We can also use bootstrapping to estimate the prediction error using a <em>leave-one-out</em> technique:\n",
      "$$\n",
      "\\mathrm{Err} = \\frac 1 N \\sum_i \\dfrac{1}{|C^{-i}|} \\sum_{b\\in C^{-i}} L(y_i, \\hat f ^b (x_i)),\n",
      "$$\n",
      "where $C^{-i}$ is the set of indices of bootstrapped samples $b$ that <em>do not</em> contain observation $i$. This seemingly complex formula is used to avoid <em>data leakage</em>: We <em>train</em> models on boostrapped data <em>not</em> containing some data, and then <em>test</em> the performance of the model against that data.</p>, '\\n', <h3>Bagging</h3>, '\\n', <p>Bagging takes the idea of bootstrapping one step further: instead of just bootstrapping data to estimate the <em>accuracy</em> of a model, we can construct an ensemble of models based on bootstrapped data to improve the estimate <em>itself</em>.</p>, '\\n', <p>In bagging, we construct $B$ models and regard our estimator as the average of all models:\n",
      "$$\n",
      "\\hat f_\\mathrm{bagged} (x) = \\frac 1 B \\sum_b \\hat f ^b (x),\n",
      "$$\n",
      "and where each model $\\hat f^b$ is learned on a bootstrapped sample. Bagging can improve estimation if $\\hat f (x)$ is a non-linear or adaptive function of the data $x$, but does not affect it if $\\hat f(x)$ depends linearly on $x$. The variance of bagged models is reduced by a term $\\mathrm{var}[ \\hat f(x) - \\langle \\hat f (x)\\rangle ]$, i.e. the in-model variance, since we average over many models.</p>, '\\n', <h3>Boosting</h3>, '\\n', <p>Conceptually, boosting consists of increasing the weights of misclassified observations, such that the model pays more 'attention' to them. It is often dramatically much better than the non-boosted model. There are however, downsides: misclassified observations deteriorate the performance of the model more so than otherwise, and it can be more computationally expensive. For the exponential loss function, there is a simple algorithm that updates the weights, which is straightforward to implement and not computationally expensive called <em>AdaBoost .M1</em>. The exponential loss function has various nice properties, e.g. the minimizer of this loss-function is equal to the log-odss of the probabilities. Apparently, the expeontnial loss function is not so robust (although I don't understand why not). For other loss functions, the algorithm can perhaps not be written in as simple a form as AdaBoost, and <em>gradient search</em> has to be used in combination with boosting to update the weights.</p>, '\\n', <h3>Decision trees</h3>, '\\n', <p>Decision trees are networks that perform classification or regression tasks. Almost always, they are binary. A node is <em>split</em> into two vertices. I think the end-nodes are called <em>leaves</em>.</p>, '\\n', <h4>Recursive binary tree</h4>, '\\n', <p>Finding a split for a rec bin tree is done by selecting the feature with the best discriminatory power at a given node.</p>, '\\n', <p><strong>Benefit 1: </strong> Easy to implement.\n",
      "<strong>Benefit 2: </strong> Does not require adaptive techniques, such as gradient descent -- learning is typically a one-step process.\n",
      "<strong>Benefit 3: </strong> Interpretable. Decision trees can be visualized on paper and understood easily.\n",
      "<strong>Benefit 4: </strong> Curse of dimensionality is overcome, as data beyond three dimensinos cannnot be visualized, but its decision tree <em>can</em>.</p>, '\\n', <p><strong>Problem 1: </strong>Easily overfits the data (high variance).\n",
      "<strong>Problem 2: </strong>Cannot describe any data space, even as the depth of the tree is increased. Non-binary trees would solve this problem, but non-binary trees apparently have other problems.</p>, '\\n', <h4>Random forests</h4>, '\\n', <p>Random forests attempt to overcome the problem of high variance by combining bagging with recursive binary trees. At each node, instead of <em>directly</em> selecting the best feature variable/point to split (as we do in the case of rec bin trees), we <em>first</em> select $m\\leq p$ features at random from the $p$ possible features. Thus, at each point, we reduce the dimensionality of space to an $\\mathbb R^m$ dimensional space. We then construct a bagged model by making such a rec bin tree $B$ times. Thus, we have two <em>hyperparameters</em>: $B$ (ensemble size) and $m$ (number of features to consider at each node).</p>, '\\n', <p>Typical values of $B$ are $\\sim 10^3$, but I suppose it depends on the size of the training data. In ELS, the author tries to find $B$ by plotting the test error vs. $B$ and then visually finding a good cut-off point.</p>, '\\n', <p>Typical values of $m$ are different between classification and regression tasks, as low as 1 and $\\sqrt p$ for classification and $p/3$ for regression.</p>, '\\n', <p>Apparently, boosted trees typically work better than random forests, although random forests are very easy to construct and also work very well.</p>, '\\n', <p>Out-of-bag sampling can (must?) be done with random forests as follows:</p>, '\\n', <blockquote>\n",
      "<p>For each observation $z_i = (x_i,y_i)$, construct its random forest predictor by averaging only those trees corresponding to bootstrapped samples in which $z_i$ <em>did not</em> appear.</p>\n",
      "</blockquote>, '\\n', <p>In this way, we can get the benefits of cross-validation simply as a by-product of the learning process!</p>, '\\n'], [<h2>Some data science Jargon @data-science</h2>, '\\n', <p><strong>Unstructured data: </strong>Data that does not fit a pre-defined data model. Examples: text, tables that are structured in different ways, audio, video, etc.\n",
      "<strong>Structured data: </strong>Data that fits a pre-defined data model. Examples: Tables, lists, streams, etc.</p>, '\\n'], [<h2>Health notes @health</h2>, '\\n', <p>Since wednesday, started sneezing a bit. Now, obviously, the corona epidemic is close to its most intense. Feeling a bit tired and not as energetic as normal (but also I've been sitting inside for the last two days, and have not interacted much with others), but otherwise doing quite fine. A bit of a runny nose. No headaches, not really coughing, no lung problems or problems with breathing. </p>, '\\n'], [<h2>to do tomorrow @journaling</h2>, '\\n', <ol>\n",
      "<li>Lyzeum lecture</li>\n",
      "<li>something outside to get fresh air and get da body movin ^^</li>\n",
      "<li>finishing the statistics lecture</li>\n",
      "<li>Bayesian lecture</li>\n",
      "<li>some nice BHIO activities</li>\n",
      "</ol>, '\\n', <h1>9.3.20</h1>, '\\n'], [<h2>To do today @journaling</h2>, '\\n', <p>Main focus for today should be to start using the RF model for the Titanic data. There are several obstacles I can foresee already there:</p>, '\\n', <ul>\n",
      "<li>categorical features, which don't directly allow for RF model (although it can be converted to numerical data using e.g. OneHot)</li>\n",
      "<li>missing data (which can be solved by imputing)</li>\n",
      "</ul>, '\\n', <p>So let's start by:</p>, '\\n', <ul>\n",
      "<li>assembling colums with only numerical data without missing features</li>\n",
      "</ul>, '\\n'], [<h2>Useful pandas commands @data-science</h2>, '\\n', <ul>\n",
      "<li><code>df.loc[locations]</code>, where <code>locations</code> is a binary array of the same length as <code>df</code></li>\n",
      "<li><code>df['column_name'].isin(iterable)</code>, where '<code>'column_name</code>' is a column in <code>df</code> and returns a boolean array</li>\n",
      "</ul>, '\\n'], [<h2>EA career meeting tonight</h2>, '\\n', <p>I'm supposed to 'host'/'moderate' the small EA career meeting this evening, but apparently not many people are joining.. Martin &amp; Andreas canceled and Jan is sick. So at most, it'll be me, Michael and Peter. How could we spend that time in a meaningful way? Here are some possibilities:\n",
      "- Trello / project master board (kind of random, and not very useful)\n",
      "- accountability partnering\n",
      "- personal constitution\n",
      "- unijour (kind of random)\n",
      "- agile cooking club?\n",
      "- how to win friends &amp; influence people\n",
      "Other possible topic (me-centered): how to start looking for jobs that have a positive impact? (we could have a nice discussion about this)\n",
      "Two lessons learned:\n",
      "- how valuable productive relationships with people are\n",
      "- only few hours of solid concentration per day\n",
      "- developer tea\n",
      "Two anti-lessons learned:\n",
      "- trying to convincing people with arguments :| (Carnegie's how to deal with people)\n",
      "- overengineer / overconstrain solutions in my life / not allowing tools in my life to be flexible\n",
      "- that people are using intellectual arguments doesn't mean they'd like to have an intellectual argument</p>, '\\n'], [<h2>Titanic data Kaggle challenge @data-science</h2>, '\\n', <p>I first performed a random forest model on the data with only passenger class, age, Parch (whatever that is) and ticket price. The model accuracy was about 58%. For a boosted decision tree (AdaBoost), the performance was 70% (substantially better). After I included sex (by first converting the string to a boolean), the random forest performed at about 83% and the AdaBoost at about 80%.</p>, '\\n'], [<h2>Impact of no. of siblings on model performance</h2>, '\\n', <p>Here's the performance <em>with</em> the number of siblings per passenger included:\n",
      "<img alt=\"20-03-09_titanic_model_performance_with_siblings\" src=\"unijour_figs/20-03-09_titanic_model_performance_with_siblings.png\"/>\n",
      "And here it is <em>excluded</em>:\n",
      "<img alt=\"20-03-09_titanic_model_performance_without_siblings\" src=\"unijour_figs/20-03-09_titanic_model_performance_without_siblings.png\"/>\n",
      "Strangely, <em>adding</em> a feature makes the performance <em>worse</em>, which I don't understand. Maybe this is because of increased variance due to adding of the feature?</p>, '\\n'], [<h2>Decision tree visualization</h2>, '\\n', <p>Here's a typical decision tree from the titanic data.\n",
      "<img alt=\"20-03-09_decision_tree_random_forest_titanic\" src=\"unijour_figs/20-03-09_decision_tree_random_forest_titanic.png\"/></p>, '\\n'], [<h2>Vier Ohren modell @journaling</h2>, '\\n', <p>Between sender and receiver, according to the 'four ears model' by von Thun, there are four different sides to every communication:</p>, '\\n', <ul>\n",
      "<li>Beziehungsseite</li>\n",
      "<li>Sachebene</li>\n",
      "<li>Appellseite</li>\n",
      "<li>Selbstkundgabe</li>\n",
      "</ul>, '\\n', <h1>10.3.20</h1>, '\\n'], [<h2>health notes, corona, cough etc @health</h2>, '\\n', <p>My health is fine again. Having a very slight cough sometimes, but basically whatever I had over the last few days seems to be over. I think it was a common cold. Never got very serious.</p>, '\\n'], [<h2>Some notes from yesterday @journaling</h2>, '\\n', <p>Yesterday was a wonderful, meaningful and productive day! I got up on time (9 AM), despite my breakfast with Valerie being canceled. I then successfully implemented a random forest model for the titanic data with 83% performance. It wasn't difficult, but now I do have the piping down for the next time that I would like to train an ML model. It's not clear to me how to improve upon this, though. I'm also not sure whether my focus should be there for the upcoming weeks, as this will be something that I'll probably learn on the job. A better focus might be to on learning a variety of ML techniques, including NNs and deep learning.</p>, '\\n', <p>Then I finished my Lyzeum lecture yesterday. I haven't yet included base rate neglect, but I think I should include it, because I might be finished early (although it's difficult to tell -- the subject material for this week (fat tails) is especially abstract). I'm really happy that I worked on that instead of just watching TV or something. What I can learn from this, is that I get much more joy and satisfaction out of doing meaningful, challenging things than dopamine drips such as YouTube. So I should remember that next time that I'm considering 'relaxing' (which often I don't really find relaxing). I actually kind of just want to cut bullshit TV out of my life. Tantalizing video essays on YouTube can be amazing and eye-opening, but random videos (especially about politics) and TV shows are not worth my time.</p>, '\\n', <p>After I finished the Lyzeum lecture, I went to Anika's to go for a Spaziergang. In the end, we never did a Spaziergang, but just stayed in and did some quatsching. Oh, the joy! I haven't laughed so much in a long time. There's always a wonderful atmosphere at the Schöttles. I can also tell that I'm really falling for Valerie, so I should communicate that to her asap. She might leave in four months from now, and I don't want to have her leave without at least giving a relationship with her a shot. </p>, '\\n', <p>However, this morning I woke up <em>way</em> too late (11.30). Don't exactly know why my body would need that amount of sleep (I was already asleep by 1 AM I think). Maybe it's also that I snoozed in the morning, creating only half-effective sleep. I'm already on the right track with going to bed on time over the last few days, but now I also want to start gettign up again consistently at 9 over the next days. So that's my goal: in bed by 24.00 at the latest, out of bed at 9 AM at the latest.</p>, '\\n'], [<h2>To do today @journaling</h2>, '\\n', <p>So what's up for today? I have one main goal in my calendar: random forest for titanic data. I think one big thing that hasn't been added yet to the titanic data is imputing missing data, which accounts for about 20% of the datapoints. I can't see how that will push the performance <em>much</em> more, but let's give it a try.</p>, '\\n', <ul>\n",
      "<li>impute data with median</li>\n",
      "<li>compare performance with imputed data</li>\n",
      "<li>check embarked features (see trello board)</li>\n",
      "</ul>, '\\n'], [<h2>Titanic data imputing data @data-science</h2>, '\\n', <p>Let's have a look at the missing data, and see whether there's any bias there one way or another. There are some small differences, which are likely due to random noise, but the biggest differences I can spot are:</p>, '\\n', <ul>\n",
      "<li>the missing data is of a lower class (average class 2.6 compared to 2.2) and cheaper ticket price (median[^why_median_for_ticket_price]</li>\n",
      "</ul>, '\\n', <p>[^why_median_for_ticket_price]:Because the ticket price is potentially unbounded, it is more insightful to use median price for the ticket, since this will not be heavily skewed by a few ultra-expensive tickets. Since class does suffer from that issue but is an integer, for the class the average is more insightful.</p>, '\\n', <p>price 8 compared to 16).</p>, '\\n', <p><img alt=\"20-03-10_titanic_summary_wo_nans\" src=\"unijour_figs/20-03-10_titanic_summary_wo_nans.png\" style=\"zoom:50%;\"/></p>, '\\n', <p><img alt=\"20-03-10_titanic_summary_wo_nans\" src=\"unijour_figs/20-03-10_titanic_summary_with_nans.png\" style=\"zoom:50%;\"/></p>, '\\n', <p>How should we incorprate this information? This is not a trivial problem, and it is an iterative one I think, as we should somehow guess the age of the missing features conditioned on the relevant subcategory. For now, however, let's <em>pretend</em> that age is not coupled to ticket fare etc. (even though it is), and just fill in the median.</p>, '\\n'], [<h2>Titanic data with imputed data</h2>, '\\n', <p>With imputing the age according to the train, test median, the RF model basically does exactly as well (no visual difference observable):</p>, '\\n', <p><img alt=\"20-03-10_perf_titanic_data_w_imputing\" src=\"unijour_figs/20-03-10_perf_titanic_data_w_imputing.png\"/></p>, '\\n', <p>Of course, this could be the case because by imputing the data according to the whole-group median, we are doing two things simultaneously:</p>, '\\n', <ul>\n",
      "<li><strong>biasing</strong> the missing features towards the fully entered data</li>\n",
      "<li><strong>debiasing</strong> the presence of non-imputed features, since those non-imputed features might be systematically underrepresented if we remove the entries with missing features alltogether</li>\n",
      "</ul>, '\\n', <p>Thus, there is maybe some kind of trade-off between the <em>debiasing</em> effect of <em>missing</em> features because we were removing entries, and the <em>biasing</em> effect of <em>aligning</em> the missing features with their population averaged values.</p>, '\\n'], [<h2>\"Embarked\" in the data</h2>, '\\n', <ul>\n",
      "<li>is the \"embarked\" feature always S,C or Q?  -&gt; yes</li>\n",
      "<li>what is \"embarked\"? it's a point of embarking on the titanic, so I don't see any prior reason why this would make the model more predictive, although it might correlate with position on the ship (which will probably correlate with survival probability)</li>\n",
      "</ul>, '\\n', <p>I added a one-hot encoding to the \"embarked\" feature and trained random forests on that. Turns out that my suspicion was correct -- adding the point of departure for the travelers does not improve predictive power:</p>, '\\n', <p><img alt=\"20-03-10_titanic_random_forest_w_one_hot_encodign\" src=\"unijour_figs/20-03-10_titanic_random_forest_w_one_hot_encodign.png\" style=\"zoom:50%;\"/></p>, '\\n'], [<h2>Position on ship in data</h2>, '\\n', <p>Can we do <em>feature engineering</em>  by adding position on the ship? One way to do this is to directly use the cabin, but there are so many different categories there, that this becomes intractable (surprisingly, for most cabins there's only person per cabin). However, there's a clear layout for the titanic based on cabin ID:</p>, '\\n', <p><img alt=\"20-03-10_titanic_layout\" src=\"unijour_figs/20-03-10_titanic_layout.png\" style=\"zoom:50%;\"/></p>, '\\n', <p>Let's start by using only the first letter as a category. There are strings 'NaN' in the cabin column, but for now I'll treat them as a separate category, assuming that people with a 'NaN' category are maybe people that didn't book a ticket in advance or something. I therefore converted the <code>np.nan</code> to a string 'NaN' and then took the first letter as a category.</p>, '\\n', <p>The performance with the cabin letter included is, again, identical to the performance without:</p>, '\\n', <p><img alt=\"20-03-10_w_cabin_class\" src=\"unijour_figs/20-03-10_w_cabin_class.png\" style=\"zoom:50%;\"/></p>, '\\n', <p>Why does the performance seem to be limited to 82%?</p>, '\\n'], [<h2>Summary of Meeting with Evgeny</h2>, '\\n', <p>Had quite an intense meeting with Evgeny today. Here are some notes:</p>, '\\n', <h3>Boosting</h3>, '\\n', <ul>\n",
      "<li>in general:  simply the idea of taking many weak learners to have their ensemble output be a strong learner</li>\n",
      "<li>usually: we adapt the weights of the datapoints who are often misclassified to give them more weight.</li>\n",
      "</ul>, '\\n', <p>AdaBoost uses the exponential loss function for the performance of the full model $f(x_i)$ at a specific datapoint $x_i$ with respect to the label $y_i$. Then, the step size in updating the weights of both the datapoints and the individual learners (the 'base functions') can be analytically found. In general, boosting can be formulated as a minimization problem over a high-dimensional space, whereby each dimension corresponds to a different sample in the training data. We do this by adding a new learner that tries to minimize the total error, but the total error is the <em>weighted</em> sum of misclassified points, and the 'difficult' to classifcy samples (say, $x^<em>$) get a higher weight. This way, the next learner 'learns' from the previous mistake of misclassifying datapoint $x^</em>$, because there's a higher penalty $w^* &gt; \\overline w$ (where $\\overline w$ is the average weight of misclassifying a datapoint) associated with misclassifying that particular datapoint.</p>, '\\n', <p>For the very first weak learner during boosting, the model is a simple weak learner. We leave that weak learner in the ensemble, but it's probably not very good. After the model has been trained however, we rescale the weights in the cost function so that the misclassified points have a higher weight. The second weak learner in our ensemble will now pay more attention to those misclassified points. Each learner gets is own weight based on how good its predictions were: the better its predictions (for all the data), the higher its weight will be. Thus, boosting combines two ideas:</p>, '\\n', <ul>\n",
      "<li>learners that were correct more often get a higher weight in the ensemble</li>\n",
      "<li>training samples that were misclassified more often get a higher cost </li>\n",
      "</ul>, '\\n', <h3>General communication with Evgeny</h3>, '\\n', <p>I noticed that I'm maybe trying to be a bit too fast... Not very often, but a few times, I had to be corrected by Evgeny. Obviously, I'm learning this stuff, so that's not a big deal, but I think it'd be nice to learn to express uncertainty better than I do now. I've actually heard this before -- that I appear very confident. But, as of yet, I can't really regulate this properly, whereas I should maybe learn to express uncertainty better.</p>, '\\n', <h3>Evgeny's tips for my Kaggle competition</h3>, '\\n', <ul>\n",
      "<li>make whole jupyter notebook into a single function</li>\n",
      "<li>use that single function for model generation to do some model comparison</li>\n",
      "<li>make some more interpretable predictions such as \"is it better to be a man or a woman aboard the Titanic\"</li>\n",
      "</ul>, '\\n', <h1>11.3.20</h1>, '\\n'], [<h2>Notes after Lyzeum class @critical-thinking</h2>, '\\n', <p>The class of today was about estimating extremes, and we discussed the property of fat tailedness. The class was okay, but definitely could have been much better. The kids didn't really know what a histogram was, and I kind of went ahead without establishing that before moving on. So I think I spent about 20-30 minutes due to the confusion about what a histogram was, and how that's supposed to be read. In particular, Anatoly didn't really get what a histogram was for much of the initial discussion. Looking back, I think this is was because I'm overestimating the ability of the students or underestimating the difficulty of the topic (I think especially the second one). The students are really smart and are quick in catching on, once they get 50% of the topic, and then they also really want to understand. But the topic today contained several layers, which I kind of glossed over (and this bit me in the ass in the end):</p>, '\\n', <ul>\n",
      "<li>how to properly read a histogram</li>\n",
      "<li>how to define an average based off of the histogram</li>\n",
      "<li>how to define limits and how this is necessary for the average of a distribution that extends into infinity</li>\n",
      "<li>the mismatch between the moment of a model distribution and that nothing in real life can be infinity</li>\n",
      "<li>that indeed the average can tend to inifinity, and what this means for the average of the underlying real data</li>\n",
      "</ul>, '\\n', <p>So even though I wasn't planning on this, I do think I have to come back to the question of how this average can become infinity by using a simpler approach:</p>, '\\n', <ol>\n",
      "<li>saying that I'll sample numbers from a distribution</li>\n",
      "<li>then plot sticks whose length is equal to the ranom variable</li>\n",
      "<li>show that for a few samples, the average stick length is finite</li>\n",
      "<li>show that for many samples (this'll become a stick chart), the average stick length becomes arbitrarily large</li>\n",
      "</ol>, '\\n', <p>Also, I should show this 'decision matrix' that I made.</p>, '\\n', <p>Also, in general, here are few things I noticed from the class today:</p>, '\\n', <ul>\n",
      "<li>I jam way too much content into each class. I think I should reduce the content by about 20%</li>\n",
      "<li>Vicky often seems to misunderstand certain things. I should talk to her sometime soon</li>\n",
      "<li>I think I talk too much</li>\n",
      "<li>the kids seemed to really get a kick out of the Fermi calculation exercise last time, so maybe I should make more stuff like that</li>\n",
      "</ul>, '\\n'], [<h2>Live streaming the statistics lecture</h2>, '\\n', <p>I think I'll live stream the statistics lecture as follows:</p>, '\\n', <ul>\n",
      "<li>mirror my macbook screen to my ipad</li>\n",
      "<li>I can write on documents using my ipad pencil on my ipad</li>\n",
      "<li>add a little flaoting window using quicktime so that people can see my face and hear my voice</li>\n",
      "<li>livestream my screen using youtube</li>\n",
      "<li>I could read my notes using my work MacBook </li>\n",
      "</ul>, '\\n', <h1>12.3.20</h1>, '\\n'], [<h2>Feedback for Timo @journaling</h2>, '\\n', <ul>\n",
      "<li>it's quite often happened that he doesn't do something that he said he would do. Each time, he did feel he have a reason to not go / forego on his promise, but to not make good on your promise is a choice because of what happened. The other choice would have been to make good on your promise despite what happened. Examples:</li>\n",
      "<li>visiting my stat lecture. After saying several times he wanted to go, he in the end was too tired.</li>\n",
      "<li>cleaning up his office. Even though saying that he would, he then didn't.</li>\n",
      "<li>giving back things he borrowed from me e.g. in the office (like paints, brushes, but then in the end they weren't cleaned etc.)</li>\n",
      "<li>not visiting the kkk even though he said he would</li>\n",
      "<li>saying he would go do sports for himself, but I'm pretty sure he didn't :')</li>\n",
      "<li>saying he would practice his speech, but then I think he didn't really in the end</li>\n",
      "</ul>, '\\n'], [<h2>Strategy Call with Andreas</h2>, '\\n', <ul>\n",
      "<li>going to bed defintely before 24</li>\n",
      "<li>text Andreas if I'm too late</li>\n",
      "<li>daily jot-down</li>\n",
      "<li>waking up at 9</li>\n",
      "</ul>, '\\n', <h1>13.3.20</h1>, '\\n'], [<h2>Motivated Reasoning @critical-thinking @journaling</h2>, '\\n', <p>Yesterday evening I had a flash of clarity. I'm having several projects in my life, some of which are critical thinking related and some of which aren't. Why am I engaging in these things that are not primarily centered around critical thinking? It doesn't make sense. I should rather put all my guns (insofar as I can, apart from upskilling in data-science etc.) onto increasing the probability of making people think more critically. So I should start taking my YouTube channel more seriously. For now, I don't think I have time, amidst DS upskilling and hopefully starting a German course soon -- but I should <em>definitely</em> start the channel this year.</p>, '\\n'], [<h2>Borislav's comments about the statistics talk</h2>, '\\n', <p>So Borislav was quite negative in the end. I think I don't agree with him on everything, he just had different expectations than I did, but he did have some good ideas:</p>, '\\n', <ul>\n",
      "<li>when zooming in and out, it's important to recap why what we did was useful in 10 s</li>\n",
      "<li>try to get more feedback from the audience, e.g. by asking questions digitally</li>\n",
      "<li>allow for more pauses, I was basically talking all the time</li>\n",
      "<li>more frequently show the roadmap than just once maybe</li>\n",
      "</ul>, '\\n', <h1>15.3.20</h1>, '\\n'], [<h2>To do today</h2>, '\\n', <p>I'd like today to be a very productive and meaningful day. So I'd like to get some stuff done off of my BHIO plate and also (hopefully) finish the next Lyzeum lecture. Here's my to-do list for today:</p>, '\\n', <ul>\n",
      "<li>[x] buy HDMI cable for lyzeum</li>\n",
      "<li>[ ] finish laundry fro CW12 and do ironing</li>\n",
      "<li>[x] remove streamthingy account</li>\n",
      "<li>[ ] clean up stuff under office desk</li>\n",
      "<li>[x] finish grammarly of website and resume</li>\n",
      "<li>[ ] work 2 hrs on python script for creating markdown file views</li>\n",
      "<li>[ ] start with 30-45 mins on Bayesian updating talk to get it going</li>\n",
      "<li>[ ] 1 hr on how to do deliberate practice in teaching? (req reading: \"how to solve it, polia\", \"how to think about analysis, alcock\", \"a mind for numbers, Oakley\", oakley's course \"how to learn\", \"so good they can't ignore you\", \"deep work\", (\"how to win at college\" \"how to be a straight a student\"))</li>\n",
      "<li>[ ] work 2 hrs on Lyzeum</li>\n",
      "</ul>, '\\n'], [<h2>Notes from journaling with Andreas @journaling</h2>, '\\n', <ul>\n",
      "<li>I'd like to become a great critical thinker, but becoming an exceptional critical thinker (top 0.1%, say, or better than all my acquaintances) is not necessary. Rather, I'd like to become an <em>exceptionally good teacher</em>.</li>\n",
      "<li>deliberate practice is isolating a 'micro-skill' and practicing this over and over</li>\n",
      "<li>I would like to do a lot of deliberate practice on becoming an excellent teacher</li>\n",
      "<li>deliberate practice for several years can make an expert</li>\n",
      "<li>for now, I would like to focus on self-compassion and nonviolent communication, but in a few months from now I would like to start focusing more on deliberate practice in teaching</li>\n",
      "</ul>, '\\n']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "level = 2\n",
    "headings = soup.find_all(f'h{level}')\n",
    "soup.contents\n",
    "level = 2\n",
    "line_numbers = []\n",
    "for line_nr, line in enumerate(soup.contents):\n",
    "    tag = f'<h{level}>'\n",
    "#     print(str(line))\n",
    "    if tag == str(line)[:len(tag)]:\n",
    "        line_numbers += [line_nr]\n",
    "all_heading_contents = []\n",
    "for line_nr_start, line_nr_end in zip(line_numbers[:-1], line_numbers[1:]):\n",
    "    all_heading_contents += [soup.contents[line_nr_start:line_nr_end]]\n",
    "\n",
    "\n",
    "print( all_heading_contents )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['@journaling','@critical-thinking', '@data-science']\n",
    "tag_dict = { tag: [] for tag in tags }\n",
    "for tag in tag_dict.keys():\n",
    "\n",
    "    for it, contents in enumerate(all_heading_contents):\n",
    "#         print(contents[0])\n",
    "#         print(type(contents[0]))\n",
    "        if tag in str(contents[0]):\n",
    "            tag_dict[tag] += [str(el) for el in contents]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag, contents in tag_dict.items():\n",
    "    with open(f'{tag}.html', mode='w') as File:\n",
    "        File.write( str([line for line in contents]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
